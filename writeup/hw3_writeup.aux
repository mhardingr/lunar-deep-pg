\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces REINFORCE Algorithm without baseline: learning curve - Every k=500 episodes the current policy is tested on 100 episodes. The plot shows the mean and standard deviation of each of this tests. }}{1}{figure.1}}
\newlabel{1}{{1}{1}{REINFORCE Algorithm without baseline: learning curve - Every k=500 episodes the current policy is tested on 100 episodes. The plot shows the mean and standard deviation of each of this tests}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A2C Implementation}}{2}{table.1}}
\newlabel{table}{{1}{2}{A2C Implementation}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A2C Algorithm N=1: learning curve. Wider critic network (30x30x30, relu activations for hidden layers, linear activation for output layer), critic lr = 0.001, actor lr = 0.001. First reached a mean reward of 225 after 20500 episodes. }}{3}{figure.2}}
\newlabel{2}{{2}{3}{A2C Algorithm N=1: learning curve. Wider critic network (30x30x30, relu activations for hidden layers, linear activation for output layer), critic lr = 0.001, actor lr = 0.001. First reached a mean reward of 225 after 20500 episodes}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A2C Algorithm N=20: learning curve. Actor and critic lr = 0.001. First reached a mean reward of 200 after 6500 episodes. }}{3}{figure.3}}
\newlabel{3}{{3}{3}{A2C Algorithm N=20: learning curve. Actor and critic lr = 0.001. First reached a mean reward of 200 after 6500 episodes}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A2C Algorithm N=50: learning curve. Actor and critic lr = 0.001. First reached a mean reward of 201 after 20500 episodes. }}{4}{figure.4}}
\newlabel{4}{{4}{4}{A2C Algorithm N=50: learning curve. Actor and critic lr = 0.001. First reached a mean reward of 201 after 20500 episodes}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A2C Algorithm N=100: learning curve. Actor lr=0.0008 and critic lr=0.001. First reached a mean reward of 200 after 25500 episodes. }}{4}{figure.5}}
\newlabel{5}{{5}{4}{A2C Algorithm N=100: learning curve. Actor lr=0.0008 and critic lr=0.001. First reached a mean reward of 200 after 25500 episodes}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A2C Algorithm N=100: 2nd learning curve showing slower convergence. Actor lr=0.001 and critic lr=0.001. First reached a mean reward of 204 after 32000 episodes. }}{5}{figure.6}}
\newlabel{6}{{6}{5}{A2C Algorithm N=100: 2nd learning curve showing slower convergence. Actor lr=0.001 and critic lr=0.001. First reached a mean reward of 204 after 32000 episodes}{figure.6}{}}
